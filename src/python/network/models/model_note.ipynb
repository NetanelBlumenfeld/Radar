{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import math\n",
    "from torchsummary import summary\n",
    "from super_resolution.srrnet import SRResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(10, 3, 16, 16)\n",
    "model = SRResNet()\n",
    "res = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from utils import ImageTransforms\n",
    "\n",
    "\n",
    "class SRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to be used by a PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder, split, crop_size, scaling_factor, lr_img_type, hr_img_type, test_data_name=None):\n",
    "        \"\"\"\n",
    "        :param data_folder: # folder with JSON data files\n",
    "        :param split: one of 'train' or 'test'\n",
    "        :param crop_size: crop size of target HR images\n",
    "        :param scaling_factor: the input LR images will be downsampled from the target HR images by this factor; the scaling done in the super-resolution\n",
    "        :param lr_img_type: the format for the LR image supplied to the model; see convert_image() in utils.py for available formats\n",
    "        :param hr_img_type: the format for the HR image supplied to the model; see convert_image() in utils.py for available formats\n",
    "        :param test_data_name: if this is the 'test' split, which test dataset? (for example, \"Set14\")\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_folder = data_folder\n",
    "        self.split = split.lower()\n",
    "        self.crop_size = int(crop_size)\n",
    "        self.scaling_factor = int(scaling_factor)\n",
    "        self.lr_img_type = lr_img_type\n",
    "        self.hr_img_type = hr_img_type\n",
    "        self.test_data_name = test_data_name\n",
    "\n",
    "        assert self.split in {'train', 'test'}\n",
    "        if self.split == 'test' and self.test_data_name is None:\n",
    "            raise ValueError(\"Please provide the name of the test dataset!\")\n",
    "        assert lr_img_type in {'[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet-norm'}\n",
    "        assert hr_img_type in {'[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet-norm'}\n",
    "\n",
    "        # If this is a training dataset, then crop dimensions must be perfectly divisible by the scaling factor\n",
    "        # (If this is a test dataset, images are not cropped to a fixed size, so this variable isn't used)\n",
    "        if self.split == 'train':\n",
    "            assert self.crop_size % self.scaling_factor == 0, \"Crop dimensions are not perfectly divisible by scaling factor! This will lead to a mismatch in the dimensions of the original HR patches and their super-resolved (SR) versions!\"\n",
    "\n",
    "        # Read list of image-paths\n",
    "        if self.split == 'train':\n",
    "            with open(os.path.join(data_folder, 'train_images.json'), 'r') as j:\n",
    "                self.images = json.load(j)\n",
    "        else:\n",
    "            with open(os.path.join(data_folder, self.test_data_name + '_test_images.json'), 'r') as j:\n",
    "                self.images = json.load(j)\n",
    "\n",
    "        # Select the correct set of transforms\n",
    "        self.transform = ImageTransforms(split=self.split,\n",
    "                                         crop_size=self.crop_size,\n",
    "                                         scaling_factor=self.scaling_factor,\n",
    "                                         lr_img_type=self.lr_img_type,\n",
    "                                         hr_img_type=self.hr_img_type)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        This method is required to be defined for use in the PyTorch DataLoader.\n",
    "\n",
    "        :param i: index to retrieve\n",
    "        :return: the 'i'th pair LR and HR images to be fed into the model\n",
    "        \"\"\"\n",
    "        # Read image\n",
    "        img = Image.open(self.images[i], mode='r')\n",
    "        img = img.convert('RGB')\n",
    "        if img.width <= 96 or img.height <= 96:\n",
    "            print(self.images[i], img.width, img.height)\n",
    "        lr_img, hr_img = self.transform(img)\n",
    "\n",
    "        return lr_img, hr_img\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This method is required to be defined for use in the PyTorch DataLoader.\n",
    "\n",
    "        :return: size of this data (in number of images)\n",
    "        \"\"\"\n",
    "        return len(self.images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
